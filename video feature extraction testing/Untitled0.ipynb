{
 "metadata": {
  "name": "",
  "signature": "sha256:0bcf0d31b99c1214c564732c39a854e2066bce0fdc4aec10ee1a927f54bba12a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "# See also: http://sundararajana.blogspot.com/2007/05/motion-detection-using-opencv.html\n",
      "\n",
      "import cv2 as cv\n",
      "import time\n",
      "\n",
      "from scipy import *\n",
      "from scipy.cluster import vq\n",
      "import numpy\n",
      "import sys, os, random, hashlib\n",
      "\n",
      "from math import *\n",
      "\n",
      "\"\"\"\n",
      "Python Motion Tracker\n",
      "\n",
      "Reads an incoming video stream and tracks motion in real time.\n",
      "Detected motion events are logged to a text file.  Also has face detection.\n",
      "\"\"\"\n",
      "\n",
      "#\n",
      "# BBoxes must be in the format:\n",
      "# ( (topleft_x), (topleft_y) ), ( (bottomright_x), (bottomright_y) ) )\n",
      "top = 0\n",
      "bottom = 1\n",
      "left = 0\n",
      "right = 1\n",
      "\n",
      "def merge_collided_bboxes( bbox_list ):\n",
      "\t# For every bbox...\n",
      "\tfor this_bbox in bbox_list:\n",
      "\t\t\n",
      "\t\t# Collision detect every other bbox:\n",
      "\t\tfor other_bbox in bbox_list:\n",
      "\t\t\tif this_bbox is other_bbox: continue  # Skip self\n",
      "\t\t\t\n",
      "\t\t\t# Assume a collision to start out with:\n",
      "\t\t\thas_collision = True\n",
      "\t\t\t\n",
      "\t\t\t# These coords are in screen coords, so > means \n",
      "\t\t\t# \"lower than\" and \"further right than\".  And < \n",
      "\t\t\t# means \"higher than\" and \"further left than\".\n",
      "\t\t\t\n",
      "\t\t\t# We also inflate the box size by 10% to deal with\n",
      "\t\t\t# fuzziness in the data.  (Without this, there are many times a bbox\n",
      "\t\t\t# is short of overlap by just one or two pixels.)\n",
      "\t\t\tif (this_bbox[bottom][0]*1.1 < other_bbox[top][0]*0.9): has_collision = False\n",
      "\t\t\tif (this_bbox[top][0]*.9 > other_bbox[bottom][0]*1.1): has_collision = False\n",
      "\t\t\t\n",
      "\t\t\tif (this_bbox[right][1]*1.1 < other_bbox[left][1]*0.9): has_collision = False\n",
      "\t\t\tif (this_bbox[left][1]*0.9 > other_bbox[right][1]*1.1): has_collision = False\n",
      "\t\t\t\n",
      "\t\t\tif has_collision:\n",
      "\t\t\t\t# merge these two bboxes into one, then start over:\n",
      "\t\t\t\ttop_left_x = min( this_bbox[left][0], other_bbox[left][0] )\n",
      "\t\t\t\ttop_left_y = min( this_bbox[left][1], other_bbox[left][1] )\n",
      "\t\t\t\tbottom_right_x = max( this_bbox[right][0], other_bbox[right][0] )\n",
      "\t\t\t\tbottom_right_y = max( this_bbox[right][1], other_bbox[right][1] )\n",
      "\t\t\t\t\n",
      "\t\t\t\tnew_bbox = ( (top_left_x, top_left_y), (bottom_right_x, bottom_right_y) )\n",
      "\t\t\t\t\n",
      "\t\t\t\tbbox_list.remove( this_bbox )\n",
      "\t\t\t\tbbox_list.remove( other_bbox )\n",
      "\t\t\t\tbbox_list.append( new_bbox )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Start over with the new list:\n",
      "\t\t\t\treturn merge_collided_bboxes( bbox_list )\n",
      "\t\n",
      "\t# When there are no collions between boxes, return that list:\n",
      "\treturn bbox_list\n",
      "\n",
      "\n",
      "def detect_faces( image, haar_cascade, mem_storage ):\n",
      "\n",
      "\tfaces = []\n",
      "\timage_size = cv.GetSize( image )\n",
      "\n",
      "\t#faces = cv.HaarDetectObjects(grayscale, haar_cascade, storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, (20, 20) )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, storage )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, mem_storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, ( 16, 16 ) )\n",
      "\t#faces = cv.HaarDetectObjects(image, haar_cascade, mem_storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, ( 4,4 ) )\n",
      "\tfaces = cv.HaarDetectObjects(image, haar_cascade, mem_storage, 1.2, 2, cv.CV_HAAR_DO_CANNY_PRUNING, ( image_size[0]/10, image_size[1]/10) )\n",
      "\t\n",
      "\tfor face in faces:\n",
      "\t\tbox = face[0]\n",
      "\t\tcv.Rectangle(image, ( box[0], box[1] ),\n",
      "\t\t\t( box[0] + box[2], box[1] + box[3]), cv.RGB(255, 0, 0), 1, 8, 0)\n",
      "\n",
      "\n",
      "class Target:\n",
      "\tdef __init__(self):\n",
      "\t\t\n",
      "\t\tif len( sys.argv ) > 1:\n",
      "\t\t\tself.writer = None\n",
      "\t\t\tself.capture = cv.CaptureFromFile( sys.argv[1] )\n",
      "\t\t\tframe = cv.QueryFrame(self.capture)\n",
      "\t\t\tframe_size = cv.GetSize(frame)\n",
      "\t\telse:\n",
      "\t\t\tfps=15\n",
      "\t\t\tis_color = True\n",
      "\n",
      "\t\t\tself.capture = cv.CaptureFromCAM(0)\n",
      "\t\t\t#cv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_WIDTH, 640 );\n",
      "\t\t\t#cv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_HEIGHT, 480 );\n",
      "\t\t\tcv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_WIDTH, 320 );\n",
      "\t\t\tcv.SetCaptureProperty( self.capture, cv.CV_CAP_PROP_FRAME_HEIGHT, 240 );\n",
      "\t\t\tframe = cv.QueryFrame(self.capture)\n",
      "\t\t\tframe_size = cv.GetSize(frame)\n",
      "\t\t\t\n",
      "\t\t\tself.writer = None\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"/dev/shm/test1.mp4\", cv.CV_FOURCC('D', 'I', 'V', 'X'), fps, frame_size, is_color )\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"test2.mpg\", cv.CV_FOURCC('P', 'I', 'M', '1'), fps, frame_size, is_color )\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"test3.mp4\", cv.CV_FOURCC('D', 'I', 'V', 'X'), fps, cv.GetSize(frame), is_color )\n",
      "\t\t\t#self.writer = cv.CreateVideoWriter(\"test4.mpg\", cv.CV_FOURCC('P', 'I', 'M', '1'), fps, (320, 240), is_color )\n",
      "\t\t\t\n",
      "\t\t\t# These both gave no error message, but saved no file:\n",
      "\t\t\t###self.writer = cv.CreateVideoWriter(\"test5.h263i\", cv.CV_FOURCC('I', '2', '6', '3'), fps, cv.GetSize(frame), is_color )\n",
      "\t\t\t###self.writer = cv.CreateVideoWriter(\"test6.fli\",   cv.CV_FOURCC('F', 'L', 'V', '1'), fps, cv.GetSize(frame), is_color )\n",
      "\t\t\t# Can't play this one:\n",
      "\t\t\t###self.writer = cv.CreateVideoWriter(\"test7.mp4\",   cv.CV_FOURCC('D', 'I', 'V', '3'), fps, cv.GetSize(frame), is_color )\n",
      "\n",
      "\t\t# 320x240 15fpx in DIVX is about 4 gigs per day.\n",
      "\n",
      "\t\tframe = cv.QueryFrame(self.capture)\n",
      "\t\tcv.NamedWindow(\"Target\", 1)\n",
      "\t\t#cv.NamedWindow(\"Target2\", 1)\n",
      "\t\t\n",
      "\n",
      "\tdef run(self):\n",
      "\t\t# Initialize\n",
      "\t\t#log_file_name = \"tracker_output.log\"\n",
      "\t\t#log_file = file( log_file_name, 'a' )\n",
      "\t\t\n",
      "\t\tframe = cv.QueryFrame( self.capture )\n",
      "\t\tframe_size = cv.GetSize( frame )\n",
      "\t\t\n",
      "\t\t# Capture the first frame from webcam for image properties\n",
      "\t\tdisplay_image = cv.QueryFrame( self.capture )\n",
      "\t\t\n",
      "\t\t# Greyscale image, thresholded to create the motion mask:\n",
      "\t\tgrey_image = cv.CreateImage( cv.GetSize(frame), cv.IPL_DEPTH_8U, 1 )\n",
      "\t\t\n",
      "\t\t# The RunningAvg() function requires a 32-bit or 64-bit image...\n",
      "\t\trunning_average_image = cv.CreateImage( cv.GetSize(frame), cv.IPL_DEPTH_32F, 3 )\n",
      "\t\t# ...but the AbsDiff() function requires matching image depths:\n",
      "\t\trunning_average_in_display_color_depth = cv.CloneImage( display_image )\n",
      "\t\t\n",
      "\t\t# RAM used by FindContours():\n",
      "\t\tmem_storage = cv.CreateMemStorage(0)\n",
      "\t\t\n",
      "\t\t# The difference between the running average and the current frame:\n",
      "\t\tdifference = cv.CloneImage( display_image )\n",
      "\t\t\n",
      "\t\ttarget_count = 1\n",
      "\t\tlast_target_count = 1\n",
      "\t\tlast_target_change_t = 0.0\n",
      "\t\tk_or_guess = 1\n",
      "\t\tcodebook=[]\n",
      "\t\tframe_count=0\n",
      "\t\tlast_frame_entity_list = []\n",
      "\t\t\n",
      "\t\tt0 = time.time()\n",
      "\t\t\n",
      "\t\t# For toggling display:\n",
      "\t\timage_list = [ \"camera\", \"difference\", \"threshold\", \"display\", \"faces\" ]\n",
      "\t\timage_index = 0   # Index into image_list\n",
      "\t\n",
      "\t\n",
      "\t\t# Prep for text drawing:\n",
      "\t\ttext_font = cv.InitFont(cv.CV_FONT_HERSHEY_COMPLEX, .5, .5, 0.0, 1, cv.CV_AA )\n",
      "\t\ttext_coord = ( 5, 15 )\n",
      "\t\ttext_color = cv.CV_RGB(255,255,255)\n",
      "\n",
      "\t\t###############################\n",
      "\t\t### Face detection stuff\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_default.xml' )\n",
      "\t\thaar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_alt.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_alt2.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_mcs_mouth.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_eye.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_frontalface_alt_tree.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_upperbody.xml' )\n",
      "\t\t#haar_cascade = cv.Load( 'haarcascades/haarcascade_profileface.xml' )\n",
      "\t\t\n",
      "\t\t# Set this to the max number of targets to look for (passed to k-means):\n",
      "\t\tmax_targets = 3\n",
      "\t\t\n",
      "\t\twhile True:\n",
      "\t\t\t\n",
      "\t\t\t# Capture frame from webcam\n",
      "\t\t\tcamera_image = cv.QueryFrame( self.capture )\n",
      "\t\t\t\n",
      "\t\t\tframe_count += 1\n",
      "\t\t\tframe_t0 = time.time()\n",
      "\t\t\t\n",
      "\t\t\t# Create an image with interactive feedback:\n",
      "\t\t\tdisplay_image = cv.CloneImage( camera_image )\n",
      "\t\t\t\n",
      "\t\t\t# Create a working \"color image\" to modify / blur\n",
      "\t\t\tcolor_image = cv.CloneImage( display_image )\n",
      "\n",
      "\t\t\t# Smooth to get rid of false positives\n",
      "\t\t\tcv.Smooth( color_image, color_image, cv.CV_GAUSSIAN, 19, 0 )\n",
      "\t\t\t\n",
      "\t\t\t# Use the Running Average as the static background\t\t\t\n",
      "\t\t\t# a = 0.020 leaves artifacts lingering way too long.\n",
      "\t\t\t# a = 0.320 works well at 320x240, 15fps.  (1/a is roughly num frames.)\n",
      "\t\t\tcv.RunningAvg( color_image, running_average_image, 0.320, None )\n",
      "\t\t\t\n",
      "\t\t\t# Convert the scale of the moving average.\n",
      "\t\t\tcv.ConvertScale( running_average_image, running_average_in_display_color_depth, 1.0, 0.0 )\n",
      "\t\t\t\n",
      "\t\t\t# Subtract the current frame from the moving average.\n",
      "\t\t\tcv.AbsDiff( color_image, running_average_in_display_color_depth, difference )\n",
      "\t\t\t\n",
      "\t\t\t# Convert the image to greyscale.\n",
      "\t\t\tcv.CvtColor( difference, grey_image, cv.CV_RGB2GRAY )\n",
      "\n",
      "\t\t\t# Threshold the image to a black and white motion mask:\n",
      "\t\t\tcv.Threshold( grey_image, grey_image, 2, 255, cv.CV_THRESH_BINARY )\n",
      "\t\t\t# Smooth and threshold again to eliminate \"sparkles\"\n",
      "\t\t\tcv.Smooth( grey_image, grey_image, cv.CV_GAUSSIAN, 19, 0 )\n",
      "\t\t\tcv.Threshold( grey_image, grey_image, 240, 255, cv.CV_THRESH_BINARY )\n",
      "\t\t\t\n",
      "\t\t\tgrey_image_as_array = numpy.asarray( cv.GetMat( grey_image ) )\n",
      "\t\t\tnon_black_coords_array = numpy.where( grey_image_as_array > 3 )\n",
      "\t\t\t# Convert from numpy.where()'s two separate lists to one list of (x, y) tuples:\n",
      "\t\t\tnon_black_coords_array = zip( non_black_coords_array[1], non_black_coords_array[0] )\n",
      "\t\t\t\n",
      "\t\t\tpoints = []   # Was using this to hold either pixel coords or polygon coords.\n",
      "\t\t\tbounding_box_list = []\n",
      "\n",
      "\t\t\t# Now calculate movements using the white pixels as \"motion\" data\n",
      "\t\t\tcontour = cv.FindContours( grey_image, mem_storage, cv.CV_RETR_CCOMP, cv.CV_CHAIN_APPROX_SIMPLE )\n",
      "\t\t\t\n",
      "\t\t\twhile contour:\n",
      "\t\t\t\t\n",
      "\t\t\t\tbounding_rect = cv.BoundingRect( list(contour) )\n",
      "\t\t\t\tpoint1 = ( bounding_rect[0], bounding_rect[1] )\n",
      "\t\t\t\tpoint2 = ( bounding_rect[0] + bounding_rect[2], bounding_rect[1] + bounding_rect[3] )\n",
      "\t\t\t\t\n",
      "\t\t\t\tbounding_box_list.append( ( point1, point2 ) )\n",
      "\t\t\t\tpolygon_points = cv.ApproxPoly( list(contour), mem_storage, cv.CV_POLY_APPROX_DP )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# To track polygon points only (instead of every pixel):\n",
      "\t\t\t\t#points += list(polygon_points)\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Draw the contours:\n",
      "\t\t\t\t###cv.DrawContours(color_image, contour, cv.CV_RGB(255,0,0), cv.CV_RGB(0,255,0), levels, 3, 0, (0,0) )\n",
      "\t\t\t\tcv.FillPoly( grey_image, [ list(polygon_points), ], cv.CV_RGB(255,255,255), 0, 0 )\n",
      "\t\t\t\tcv.PolyLine( display_image, [ polygon_points, ], 0, cv.CV_RGB(255,255,255), 1, 0, 0 )\n",
      "\t\t\t\t#cv.Rectangle( display_image, point1, point2, cv.CV_RGB(120,120,120), 1)\n",
      "\n",
      "\t\t\t\tcontour = contour.h_next()\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t# Find the average size of the bbox (targets), then\n",
      "\t\t\t# remove any tiny bboxes (which are prolly just noise).\n",
      "\t\t\t# \"Tiny\" is defined as any box with 1/10th the area of the average box.\n",
      "\t\t\t# This reduces false positives on tiny \"sparkles\" noise.\n",
      "\t\t\tbox_areas = []\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\tbox_width = box[right][0] - box[left][0]\n",
      "\t\t\t\tbox_height = box[bottom][0] - box[top][0]\n",
      "\t\t\t\tbox_areas.append( box_width * box_height )\n",
      "\t\t\t\t\n",
      "\t\t\t\t#cv.Rectangle( display_image, box[0], box[1], cv.CV_RGB(255,0,0), 1)\n",
      "\t\t\t\n",
      "\t\t\taverage_box_area = 0.0\n",
      "\t\t\tif len(box_areas): average_box_area = float( sum(box_areas) ) / len(box_areas)\n",
      "\t\t\t\n",
      "\t\t\ttrimmed_box_list = []\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\tbox_width = box[right][0] - box[left][0]\n",
      "\t\t\t\tbox_height = box[bottom][0] - box[top][0]\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Only keep the box if it's not a tiny noise box:\n",
      "\t\t\t\tif (box_width * box_height) > average_box_area*0.1: trimmed_box_list.append( box )\n",
      "\t\t\t\n",
      "\t\t\t# Draw the trimmed box list:\n",
      "\t\t\t#for box in trimmed_box_list:\n",
      "\t\t\t#\tcv.Rectangle( display_image, box[0], box[1], cv.CV_RGB(0,255,0), 2 )\n",
      "\t\t\t\t\n",
      "\t\t\tbounding_box_list = merge_collided_bboxes( trimmed_box_list )\n",
      "\n",
      "\t\t\t# Draw the merged box list:\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\tcv.Rectangle( display_image, box[0], box[1], cv.CV_RGB(0,255,0), 1 )\n",
      "\t\t\t\n",
      "\t\t\t# Here are our estimate points to track, based on merged & trimmed boxes:\n",
      "\t\t\testimated_target_count = len( bounding_box_list )\n",
      "\t\t\t\n",
      "\t\t\t# Don't allow target \"jumps\" from few to many or many to few.\n",
      "\t\t\t# Only change the number of targets up to one target per n seconds.\n",
      "\t\t\t# This fixes the \"exploding number of targets\" when something stops moving\n",
      "\t\t\t# and the motion erodes to disparate little puddles all over the place.\n",
      "\t\t\t\n",
      "\t\t\tif frame_t0 - last_target_change_t < .350:  # 1 change per 0.35 secs\n",
      "\t\t\t\testimated_target_count = last_target_count\n",
      "\t\t\telse:\n",
      "\t\t\t\tif last_target_count - estimated_target_count > 1: estimated_target_count = last_target_count - 1\n",
      "\t\t\t\tif estimated_target_count - last_target_count > 1: estimated_target_count = last_target_count + 1\n",
      "\t\t\t\tlast_target_change_t = frame_t0\n",
      "\t\t\t\n",
      "\t\t\t# Clip to the user-supplied maximum:\n",
      "\t\t\testimated_target_count = min( estimated_target_count, max_targets )\n",
      "\t\t\t\n",
      "\t\t\t# The estimated_target_count at this point is the maximum number of targets\n",
      "\t\t\t# we want to look for.  If kmeans decides that one of our candidate\n",
      "\t\t\t# bboxes is not actually a target, we remove it from the target list below.\n",
      "\t\t\t\n",
      "\t\t\t# Using the numpy values directly (treating all pixels as points):\t\n",
      "\t\t\tpoints = non_black_coords_array\n",
      "\t\t\tcenter_points = []\n",
      "\t\t\t\n",
      "\t\t\tif len(points):\n",
      "\t\t\t\t\n",
      "\t\t\t\t# If we have all the \"target_count\" targets from last frame,\n",
      "\t\t\t\t# use the previously known targets (for greater accuracy).\n",
      "\t\t\t\tk_or_guess = max( estimated_target_count, 1 )  # Need at least one target to look for.\n",
      "\t\t\t\tif len(codebook) == estimated_target_count: \n",
      "\t\t\t\t\tk_or_guess = codebook\n",
      "\t\t\t\t\n",
      "\t\t\t\t#points = vq.whiten(array( points ))  # Don't do this!  Ruins everything.\n",
      "\t\t\t\tcodebook, distortion = vq.kmeans( array( points ), k_or_guess )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Convert to tuples (and draw it to screen)\n",
      "\t\t\t\tfor center_point in codebook:\n",
      "\t\t\t\t\tcenter_point = ( int(center_point[0]), int(center_point[1]) )\n",
      "\t\t\t\t\tcenter_points.append( center_point )\n",
      "\t\t\t\t\t#cv.Circle(display_image, center_point, 10, cv.CV_RGB(255, 0, 0), 2)\n",
      "\t\t\t\t\t#cv.Circle(display_image, center_point, 5, cv.CV_RGB(255, 0, 0), 3)\n",
      "\t\t\t\n",
      "\t\t\t# Now we have targets that are NOT computed from bboxes -- just\n",
      "\t\t\t# movement weights (according to kmeans).  If any two targets are\n",
      "\t\t\t# within the same \"bbox count\", average them into a single target.  \n",
      "\t\t\t#\n",
      "\t\t\t# (Any kmeans targets not within a bbox are also kept.)\n",
      "\t\t\ttrimmed_center_points = []\n",
      "\t\t\tremoved_center_points = []\n",
      "\t\t\t\t\t\t\n",
      "\t\t\tfor box in bounding_box_list:\n",
      "\t\t\t\t# Find the centers within this box:\n",
      "\t\t\t\tcenter_points_in_box = []\n",
      "\t\t\t\t\n",
      "\t\t\t\tfor center_point in center_points:\n",
      "\t\t\t\t\tif\tcenter_point[0] < box[right][0] and center_point[0] > box[left][0] and \\\n",
      "\t\t\t\t\t\tcenter_point[1] < box[bottom][1] and center_point[1] > box[top][1] :\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t# This point is within the box.\n",
      "\t\t\t\t\t\tcenter_points_in_box.append( center_point )\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Now see if there are more than one.  If so, merge them.\n",
      "\t\t\t\tif len( center_points_in_box ) > 1:\n",
      "\t\t\t\t\t# Merge them:\n",
      "\t\t\t\t\tx_list = y_list = []\n",
      "\t\t\t\t\tfor point in center_points_in_box:\n",
      "\t\t\t\t\t\tx_list.append(point[0])\n",
      "\t\t\t\t\t\ty_list.append(point[1])\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\taverage_x = int( float(sum( x_list )) / len( x_list ) )\n",
      "\t\t\t\t\taverage_y = int( float(sum( y_list )) / len( y_list ) )\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\ttrimmed_center_points.append( (average_x, average_y) )\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t# Record that they were removed:\n",
      "\t\t\t\t\tremoved_center_points += center_points_in_box\n",
      "\t\t\t\t\t\n",
      "\t\t\t\tif len( center_points_in_box ) == 1:\n",
      "\t\t\t\t\ttrimmed_center_points.append( center_points_in_box[0] ) # Just use it.\n",
      "\t\t\t\n",
      "\t\t\t# If there are any center_points not within a bbox, just use them.\n",
      "\t\t\t# (It's probably a cluster comprised of a bunch of small bboxes.)\n",
      "\t\t\tfor center_point in center_points:\n",
      "\t\t\t\tif (not center_point in trimmed_center_points) and (not center_point in removed_center_points):\n",
      "\t\t\t\t\ttrimmed_center_points.append( center_point )\n",
      "\t\t\t\n",
      "\t\t\t# Draw what we found:\n",
      "\t\t\t#for center_point in trimmed_center_points:\n",
      "\t\t\t#\tcenter_point = ( int(center_point[0]), int(center_point[1]) )\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 20, cv.CV_RGB(255, 255,255), 1)\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 15, cv.CV_RGB(100, 255, 255), 1)\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 10, cv.CV_RGB(255, 255, 255), 2)\n",
      "\t\t\t#\tcv.Circle(display_image, center_point, 5, cv.CV_RGB(100, 255, 255), 3)\n",
      "\t\t\t\n",
      "\t\t\t# Determine if there are any new (or lost) targets:\n",
      "\t\t\tactual_target_count = len( trimmed_center_points )\n",
      "\t\t\tlast_target_count = actual_target_count\n",
      "\t\t\t\n",
      "\t\t\t# Now build the list of physical entities (objects)\n",
      "\t\t\tthis_frame_entity_list = []\n",
      "\t\t\t\n",
      "\t\t\t# An entity is list: [ name, color, last_time_seen, last_known_coords ]\n",
      "\t\t\t\n",
      "\t\t\tfor target in trimmed_center_points:\n",
      "\t\t\t\n",
      "\t\t\t\t# Is this a target near a prior entity (same physical entity)?\n",
      "\t\t\t\tentity_found = False\n",
      "\t\t\t\tentity_distance_dict = {}\n",
      "\t\t\t\t\n",
      "\t\t\t\tfor entity in last_frame_entity_list:\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\tentity_coords= entity[3]\n",
      "\t\t\t\t\tdelta_x = entity_coords[0] - target[0]\n",
      "\t\t\t\t\tdelta_y = entity_coords[1] - target[1]\n",
      "\t\t\t\n",
      "\t\t\t\t\tdistance = sqrt( pow(delta_x,2) + pow( delta_y,2) )\n",
      "\t\t\t\t\tentity_distance_dict[ distance ] = entity\n",
      "\t\t\t\t\n",
      "\t\t\t\t# Did we find any non-claimed entities (nearest to furthest):\n",
      "\t\t\t\tdistance_list = entity_distance_dict.keys()\n",
      "\t\t\t\tdistance_list.sort()\n",
      "\t\t\t\t\n",
      "\t\t\t\tfor distance in distance_list:\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t# Yes; see if we can claim the nearest one:\n",
      "\t\t\t\t\tnearest_possible_entity = entity_distance_dict[ distance ]\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t# Don't consider entities that are already claimed:\n",
      "\t\t\t\t\tif nearest_possible_entity in this_frame_entity_list:\n",
      "\t\t\t\t\t\t#print \"Target %s: Skipping the one iwth distance: %d at %s, C:%s\" % (target, distance, nearest_possible_entity[3], nearest_possible_entity[1] )\n",
      "\t\t\t\t\t\tcontinue\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t#print \"Target %s: USING the one iwth distance: %d at %s, C:%s\" % (target, distance, nearest_possible_entity[3] , nearest_possible_entity[1])\n",
      "\t\t\t\t\t# Found the nearest entity to claim:\n",
      "\t\t\t\t\tentity_found = True\n",
      "\t\t\t\t\tnearest_possible_entity[2] = frame_t0  # Update last_time_seen\n",
      "\t\t\t\t\tnearest_possible_entity[3] = target  # Update the new location\n",
      "\t\t\t\t\tthis_frame_entity_list.append( nearest_possible_entity )\n",
      "\t\t\t\t\t#log_file.write( \"%.3f MOVED %s %d %d\\n\" % ( frame_t0, nearest_possible_entity[0], nearest_possible_entity[3][0], nearest_possible_entity[3][1]  ) )\n",
      "\t\t\t\t\tbreak\n",
      "\t\t\t\t\n",
      "\t\t\t\tif entity_found == False:\n",
      "\t\t\t\t\t# It's a new entity.\n",
      "\t\t\t\t\tcolor = ( random.randint(0,255), random.randint(0,255), random.randint(0,255) )\n",
      "\t\t\t\t\tname = hashlib.md5( str(frame_t0) + str(color) ).hexdigest()[:6]\n",
      "\t\t\t\t\tlast_time_seen = frame_t0\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\tnew_entity = [ name, color, last_time_seen, target ]\n",
      "\t\t\t\t\tthis_frame_entity_list.append( new_entity )\n",
      "\t\t\t\t\t#log_file.write( \"%.3f FOUND %s %d %d\\n\" % ( frame_t0, new_entity[0], new_entity[3][0], new_entity[3][1]  ) )\n",
      "\t\t\t\n",
      "\t\t\t# Now \"delete\" any not-found entities which have expired:\n",
      "\t\t\tentity_ttl = 1.0  # 1 sec.\n",
      "\t\t\t\n",
      "\t\t\tfor entity in last_frame_entity_list:\n",
      "\t\t\t\tlast_time_seen = entity[2]\n",
      "\t\t\t\tif frame_t0 - last_time_seen > entity_ttl:\n",
      "\t\t\t\t\t# It's gone.\n",
      "\t\t\t\t\t#log_file.write( \"%.3f STOPD %s %d %d\\n\" % ( frame_t0, entity[0], entity[3][0], entity[3][1]  ) )\n",
      "\t\t\t\t\tpass\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\t# Save it for next time... not expired yet:\n",
      "\t\t\t\t\tthis_frame_entity_list.append( entity )\n",
      "\t\t\t\n",
      "\t\t\t# For next frame:\n",
      "\t\t\tlast_frame_entity_list = this_frame_entity_list\n",
      "\t\t\t\n",
      "\t\t\t# Draw the found entities to screen:\n",
      "\t\t\tfor entity in this_frame_entity_list:\n",
      "\t\t\t\tcenter_point = entity[3]\n",
      "\t\t\t\tc = entity[1]  # RGB color tuple\n",
      "\t\t\t\tcv.Circle(display_image, center_point, 20, cv.CV_RGB(c[0], c[1], c[2]), 1)\n",
      "\t\t\t\tcv.Circle(display_image, center_point, 15, cv.CV_RGB(c[0], c[1], c[2]), 1)\n",
      "\t\t\t\tcv.Circle(display_image, center_point, 10, cv.CV_RGB(c[0], c[1], c[2]), 2)\n",
      "\t\t\t\tcv.Circle(display_image, center_point,  5, cv.CV_RGB(c[0], c[1], c[2]), 3)\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t#print \"min_size is: \" + str(min_size)\n",
      "\t\t\t# Listen for ESC or ENTER key\n",
      "\t\t\tc = cv.WaitKey(7) % 0x100\n",
      "\t\t\tif c == 27 or c == 10:\n",
      "\t\t\t\tbreak\n",
      "\t\t\t\n",
      "\t\t\t# Toggle which image to show\n",
      "\t\t\tif chr(c) == 'd':\n",
      "\t\t\t\timage_index = ( image_index + 1 ) % len( image_list )\n",
      "\t\t\t\n",
      "\t\t\timage_name = image_list[ image_index ]\n",
      "\t\t\t\n",
      "\t\t\t# Display frame to user\n",
      "\t\t\tif image_name == \"camera\":\n",
      "\t\t\t\timage = camera_image\n",
      "\t\t\t\tcv.PutText( image, \"Camera (Normal)\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"difference\":\n",
      "\t\t\t\timage = difference\n",
      "\t\t\t\tcv.PutText( image, \"Difference Image\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"display\":\n",
      "\t\t\t\timage = display_image\n",
      "\t\t\t\tcv.PutText( image, \"Targets (w/AABBs and contours)\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"threshold\":\n",
      "\t\t\t\t# Convert the image to color.\n",
      "\t\t\t\tcv.CvtColor( grey_image, display_image, cv.CV_GRAY2RGB )\n",
      "\t\t\t\timage = display_image  # Re-use display image here\n",
      "\t\t\t\tcv.PutText( image, \"Motion Mask\", text_coord, text_font, text_color )\n",
      "\t\t\telif image_name == \"faces\":\n",
      "\t\t\t\t# Do face detection\n",
      "\t\t\t\tdetect_faces( camera_image, haar_cascade, mem_storage )\t\t\t\t\n",
      "\t\t\t\timage = camera_image  # Re-use camera image here\n",
      "\t\t\t\tcv.PutText( image, \"Face Detection\", text_coord, text_font, text_color )\n",
      "\t\t\t\n",
      "\t\t\tcv.ShowImage( \"Target\", image )\n",
      "\t\t\t\n",
      "\t\t\tif self.writer: \n",
      "\t\t\t\tcv.WriteFrame( self.writer, image );\n",
      "\t\t\t\n",
      "\t\t\t#log_file.flush()\n",
      "\t\t\t\n",
      "\t\t\t# If only using a camera, then there is no time.sleep() needed, \n",
      "\t\t\t# because the camera clips us to 15 fps.  But if reading from a file,\n",
      "\t\t\t# we need this to keep the time-based target clipping correct:\n",
      "\t\t\tframe_t1 = time.time()\n",
      "\t\t\t\n",
      "\n",
      "\t\t\t# If reading from a file, put in a forced delay:\n",
      "\t\t\tif not self.writer:\n",
      "\t\t\t\tdelta_t = frame_t1 - frame_t0\n",
      "\t\t\t\tif delta_t < ( 1.0 / 15.0 ): time.sleep( ( 1.0 / 15.0 ) - delta_t )\n",
      "\t\t\t\n",
      "\t\tt1 = time.time()\n",
      "\t\ttime_delta = t1 - t0\n",
      "\t\tprocessed_fps = float( frame_count ) / time_delta\n",
      "\t\tprint \"Got %d frames. %.1f s. %f fps.\" % ( frame_count, time_delta, processed_fps )\n",
      "\t\t\n",
      "if __name__==\"__main__\":\n",
      "\tt = Target()\n",
      "#\timport cProfile\n",
      "#\tcProfile.run( 't.run()' )\n",
      "\tt.run()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'CaptureFromFile'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-2801b2489999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;31m#       import cProfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;31m#       cProfile.run( 't.run()' )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-3-2801b2489999>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCaptureFromFile\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQueryFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                         \u001b[0mframe_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'CaptureFromFile'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np\n",
      " \n",
      "c = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")\n",
      "frameCount=c.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT)\n",
      "print frameCount"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named cv2",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-79ea33d522bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mframeCount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCV_CAP_PROP_FRAME_COUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named cv2"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np\n",
      " \n",
      "# c = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")\n",
      "#high\n",
      "c = cv2.VideoCapture(\"video/cctv052x2004080517x01656.avi\")\n",
      "# low\n",
      "# c = cv2.VideoCapture(\"video/cctv052x2004080612x01910.avi\")\n",
      "#medium\n",
      "# c = cv2.VideoCapture(\"video/cctv052x2004080617x00076.avi\")\n",
      "\n",
      "_,f = c.read()\n",
      "restot=cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
      "avg1 = np.float32(f)\n",
      "avg2 = np.float32(f)\n",
      "back=f\n",
      "while(1):\n",
      "    _,ff = c.read()\n",
      "    f=cv2.GaussianBlur(ff,(5,5),0)\n",
      "#     cv2.accumulateWeighted(f,avg1,0.1)\n",
      "#     cv2.accumulateWeighted(f,avg2,0.01)\n",
      "    res3=cv2.absdiff(f,back)\n",
      "    \n",
      "    res4= cv2.cvtColor(res3, cv2.COLOR_BGR2GRAY)\n",
      "    edges = cv2.Canny(res4,100,200)\n",
      "   \n",
      "    res5=cv2.GaussianBlur(res4,(7,7),0)\n",
      "    (th,res5)=cv2.threshold(res5,15,255,cv2.THRESH_BINARY)\n",
      "    \n",
      "    \n",
      "#     res6=cv2.GaussianBlur(res5,(5,5),0)\n",
      "  \n",
      "    print res5\n",
      "    #cv2.threshold(res5,1)\n",
      "#     res1 = cv2.convertScaleAbs(avg1)\n",
      "#     res2 = cv2.convertScaleAbs(avg2)\n",
      "#     fgbg = cv2.BackgroundSubtractorMOG()\n",
      "#     fgmask = fgbg.apply(f)\n",
      "    cv2.imshow('img',f)\n",
      "#     cv2.imshow('avg1',restot)\n",
      "    cv2.imshow('avg2',res4)\n",
      "    cv2.imshow('diff',res5)\n",
      "    cv2.imshow('edges',edges)\n",
      "    k = cv2.waitKey(20)\n",
      "    back=f\n",
      "    if k == 27:\n",
      "        break\n",
      " \n",
      "cv2.destroyAllWindows()\n",
      "c.release()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named cv2",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-0b1a16c25b90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# c = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#high\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named cv2"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "cap = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")\n",
      " \n",
      "\n",
      "fgbg = cv2.BackgroundSubtractorMOG()\n",
      "\n",
      "while(1):\n",
      "    ret, frame = cap.read()\n",
      "\n",
      "    fgmask = fgbg.apply(frame)\n",
      "    edges = cv2.Canny(fgmask,0,200)\n",
      "    cv2.imshow('frame',fgmask)\n",
      "    cv2.imshow('edge',edges)\n",
      "    k = cv2.waitKey(30) & 0xff\n",
      "    if k == 27:\n",
      "        break\n",
      "\n",
      "cap.release()\n",
      "cv2.destroyAllWindows()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "\n",
      "def diffImg(t0, t1, t2):\n",
      "    d1 = cv2.absdiff(t2, t1)\n",
      "    d2 = cv2.absdiff(t1, t0)\n",
      "    return cv2.bitwise_and(d1, d2)\n",
      "\n",
      "cam = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")  \n",
      "\n",
      "winName = \"Movement Indicator\"\n",
      "cv2.namedWindow(winName, cv2.CV_WINDOW_AUTOSIZE)\n",
      "\n",
      "# Read three images first:\n",
      "t_minus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
      "t = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
      "t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
      "    \n",
      "while True:\n",
      "    cv2.imshow( winName, diffImg(t_minus, t, t_plus) )\n",
      "\n",
      "    # Read next image\n",
      "    t_minus = t\n",
      "    t = t_plus\n",
      "    t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
      "               \n",
      "    key = cv2.waitKey(10)\n",
      "    if key == 27:\n",
      "       cv2.destroyWindow(winName)\n",
      "       break\n",
      "    \n",
      "print \"Goodbye\"\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Goodbye\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "cap = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")\n",
      "\n",
      "# params for ShiTomasi corner detection\n",
      "feature_params = dict( maxCorners = 100,\n",
      "                       qualityLevel = 0.3,\n",
      "                       minDistance = 7,\n",
      "                       blockSize = 7 )\n",
      "\n",
      "# Parameters for lucas kanade optical flow\n",
      "lk_params = dict( winSize  = (15,15),\n",
      "                  maxLevel = 2,\n",
      "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
      "\n",
      "# Create some random colors\n",
      "color = np.random.randint(0,255,(100,3))\n",
      "\n",
      "# Take first frame and find corners in it\n",
      "ret, old_frame = cap.read()\n",
      "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
      "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
      "\n",
      "# Create a mask image for drawing purposes\n",
      "mask = np.zeros_like(old_frame)\n",
      "\n",
      "while(1):\n",
      "    ret,frame = cap.read()\n",
      "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    # calculate optical flow\n",
      "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
      "\n",
      "    # Select good points\n",
      "    good_new = p1[st==1]\n",
      "    good_old = p0[st==1]\n",
      "\n",
      "    # draw the tracks\n",
      "    for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
      "        a,b = new.ravel()\n",
      "        c,d = old.ravel()\n",
      "        mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
      "        frame = cv2.circle(frame,(a,b),5,color[i].tolist(),-1)\n",
      "    img = cv2.add(frame,mask)\n",
      "\n",
      "    cv2.imshow('frame',frame)\n",
      "    k = cv2.waitKey(30) & 0xff\n",
      "    if k == 27:\n",
      "        break\n",
      "\n",
      "    # Now update the previous frame and previous points\n",
      "    old_gray = frame_gray.copy()\n",
      "    p0 = good_new.reshape(-1,1,2)\n",
      "\n",
      "cv2.destroyAllWindows()\n",
      "cap.release()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "error",
       "evalue": "..\\..\\..\\..\\opencv\\modules\\highgui\\src\\window.cpp:261: error: (-215) size.width>0 && size.height>0 in function cv::imshow\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-0a766efa24b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31merror\u001b[0m: ..\\..\\..\\..\\opencv\\modules\\highgui\\src\\window.cpp:261: error: (-215) size.width>0 && size.height>0 in function cv::imshow\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "img = cv2.imread('messi5.jpg',0)\n",
      "edges = cv2.Canny(img,100,200)\n",
      "\n",
      "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
      "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
      "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
      "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np\n",
      " \n",
      "# c = cv2.VideoCapture(\"video & segments/traffic video/17KAJ4F_172.18.4.71_2013-06-11_06-00-00(4).mp4\")\n",
      "#high\n",
      "c1 = cv2.VideoCapture(\"video/cctv052x2004080517x01656.avi\")\n",
      "# low\n",
      "c2 = cv2.VideoCapture(\"video/cctv052x2004080612x01910.avi\")\n",
      "#medium\n",
      "c3 = cv2.VideoCapture(\"video/cctv052x2004080617x00076.avi\")\n",
      "\n",
      "_,f = c1.read()\n",
      "_,f = c2.read()\n",
      "_,f = c3.read()\n",
      "\n",
      "_,f1 = c1.read()\n",
      "_,f2 = c2.read()\n",
      "_,f3 = c3.read()\n",
      "\n",
      "vis = np.concatenate((f1, f3, f2), axis=1)\n",
      "cv2.imwrite('out.jpg', vis)\n",
      "\n",
      "\n",
      " \n",
      "cv2.destroyAllWindows()\n",
      "c.release()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import cv2\n",
      "import glob\n",
      "import ntpath\n",
      "import os.path\n",
      "\n",
      "np.set_printoptions(threshold='nan')\n",
      "\n",
      "# Take input path\n",
      "def motionFeatureExtractionStart(folder,outname):\n",
      "    max_val=0\n",
      "    L = []\n",
      "    arrayFiles = glob.glob(folder+\"/*.avi\")\n",
      "    for name in arrayFiles:\n",
      "            fname=os.path.splitext(ntpath.basename(name))[0]\n",
      "            print \"start \", name\n",
      "            cap = cv2.VideoCapture(name)\n",
      "            \n",
      "            frameCount=cap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT)\n",
      "            \n",
      "            ret, frame = cap.read()\n",
      "           \n",
      "            \n",
      "            ret, frame = cap.read()\n",
      "           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "import cv2\n",
      "#range\n",
      "n=12\n",
      "\n",
      "f=open(\"ClusterData_k3.txt\")\n",
      "l=[]\n",
      "l=[f.readline().split(',') for i in range(3)]\n",
      "nl=[]\n",
      "for i in l:\n",
      "    nl.append([r.split('_')[0]+'.avi' for r in i])\n",
      "    \n",
      "cnt=0    \n",
      "vis1=np.zeros(1)\n",
      "vis2=np.zeros(1)\n",
      "visout=np.zeros(1)\n",
      "for i in nl:\n",
      "    for x in range(4):\n",
      "        for y in range(3):\n",
      "            print i[x*4+y]\n",
      "            name=i[x*4+y]\n",
      "            cap = cv2.VideoCapture(\"video/\"+name)\n",
      "            ret, frame = cap.read()\n",
      "            ret, frame = cap.read()\n",
      "            if(ret):\n",
      "                if vis1.any():\n",
      "                    vis1 = np.concatenate((vis1, frame), axis=1)\n",
      "                else:\n",
      "                    vis1 =frame\n",
      "            \n",
      "        if vis2.any():\n",
      "            vis2=np.concatenate((vis2, vis1), axis=0)\n",
      "        else:\n",
      "            vis2=vis1\n",
      "        vis1=np.zeros(1)\n",
      "    cv2.imwrite('out'+str(cnt)+'.jpg', vis2)\n",
      "    \n",
      "    cnt=cnt+1\n",
      "    if visout.any():\n",
      "        visout=np.concatenate((visout, vis2), axis=0)\n",
      "    else:\n",
      "        visout=vis2\n",
      "    vis2=np.zeros(1)\n",
      "cv2.imwrite('outall'+str(n)+'.jpg', visout)\n",
      "\n",
      "\n",
      " \n",
      "cv2.destroyAllWindows()\n",
      "cap.release()\n",
      "\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cctv052x2004080615x00033.avi\n",
        "cctv052x2004080615x00040.avi\n",
        "cctv052x2004080616x00059.avi\n",
        "cctv052x2004080517x01661.avi\n",
        "cctv052x2004080517x01658.avi\n",
        "cctv052x2004080617x00074.avi\n",
        "cctv052x2004080517x01656.avi\n",
        "cctv052x2004080516x01646.avi\n",
        "cctv052x2004080517x01657.avi\n",
        "cctv052x2004080517x01660.avi\n",
        "cctv052x2004080615x00037.avi\n",
        "cctv052x2004080616x00056.avi\n",
        "cctv052x2004080516x01645.avi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cctv052x2004080616x00062.avi\n",
        "cctv052x2004080617x00072.avi\n",
        "cctv052x2004080616x00055.avi\n",
        "cctv052x2004080617x00070.avi\n",
        "cctv052x2004080617x00071.avi\n",
        "cctv052x2004080618x00081.avi\n",
        "cctv052x2004080516x01644.avi\n",
        "cctv052x2004080516x01650.avi\n",
        "cctv052x2004080516x01638.avi\n",
        "cctv052x2004080517x01664.avi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cctv052x2004080517x01665.avi\n",
        "cctv052x2004080606x01821.avi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cctv052x2004080609x01870.avi\n",
        "cctv052x2004080520x01700.avi\n",
        "cctv052x2004080607x01845.avi\n",
        "cctv052x2004080609x01866.avi\n",
        "cctv052x2004080610x01879.avi\n",
        "cctv052x2004080610x01883.avi\n",
        "cctv052x2004080611x01907.avi\n",
        "cctv052x2004080607x01840.avi\n",
        "cctv052x2004080614x00022.avi\n",
        "cctv052x2004080619x00095.avi\n",
        "cctv052x2004080610x01878.avi\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.read_csv(\"ClusterData_k4.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "CParserError",
       "evalue": "Error tokenizing data. C error: Expected 50 fields in line 4, saw 132\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mCParserError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-0671c526b2b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ClusterData_k4.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     infer_datetime_format=infer_datetime_format)\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m _parser_defaults = {\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    624\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas\\parser.c:7110)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas\\parser.c:7334)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas\\parser.c:7943)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas\\parser.c:7817)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Thimal\\Anaconda\\lib\\site-packages\\pandas\\parser.pyd\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas\\parser.c:19569)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;31mCParserError\u001b[0m: Error tokenizing data. C error: Expected 50 fields in line 4, saw 132\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}